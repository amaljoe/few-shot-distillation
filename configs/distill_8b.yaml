# Distillation overlay for Qwen3-8B (Condition C).
# train_layerwise_distill.py merges this on top of configs/base.yaml,
# so all 8B-specific fields must be repeated here.

model:
  name: "Qwen/Qwen3-8B"
  num_layers: 36
  hidden_size: 4096
  thinking_mode: false

training:
  output_dir: "experiments/8b"
  save_steps: 100
  max_steps: 1000

teacher_activations:
  cache_dir: "experiments/8b/teacher_cache"
  dtype: "float16"

distillation:
  lambda_distill: 0.5
  layers_to_match: "all"
  normalize_hidden: true
