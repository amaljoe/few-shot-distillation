# Early-checkpoint distillation overlay for Qwen3-8B.
# Merged on top of configs/base.yaml by train_layerwise_distill.py.

model:
  name: "Qwen/Qwen3-8B"
  num_layers: 36
  hidden_size: 4096
  thinking_mode: false

training:
  output_dir: "experiments/8b/early_checkpoints/distill"
  save_steps: 10
  max_steps: 100

teacher_activations:
  cache_dir: "experiments/8b/teacher_cache"
  dtype: "float16"

distillation:
  lambda_distill: 0.5
  layers_to_match: "all"
  normalize_hidden: true
