# Variation 2: Online all-layer all-token hidden state distillation (Qwen3-1.7B)
#
# At every answer token position and every intermediate transformer layer,
# match student hidden states to teacher hidden states (L2-normalized MSE).
# Teacher runs online (frozen, same base weights) with 8-shot context.
# No truncation to a fixed number of tokens — all answer tokens are used.
#
# Memory note: output_hidden_states=True stores all L×T×H activations.
# gradient_checkpointing MUST be false (incompatible with output_hidden_states).
#
# Run:
#   CUDA_VISIBLE_DEVICES=2,3 accelerate launch \
#     --num_processes 2 --mixed_precision bf16 --main_process_port 29501 \
#     src/training/train_online_v2.py --config configs/online_v2.yaml \
#     --output_dir experiments/online_v2

distillation:
  lambda_distill: 0.5
  normalize_hidden: true   # L2-normalize along hidden dim before MSE

training:
  output_dir: "experiments/online_v2"
  save_steps: 200
  gradient_checkpointing: false  # must be false for output_hidden_states to work
