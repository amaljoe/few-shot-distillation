## ğŸ§  Few-Shot Distillation for Stable Fine-Tuning

**Distilling In-Context Learning into Model Weights via Layer-wise Attention Supervision**

---

## ğŸ“Œ Motivation

Large Language Models often show:

* lower loss in **few-shot (in-context learning)** compared to
* **zero-shot fine-tuned models** during early training.

This suggests that:

> Context provides useful adaptation signals that weights do not initially encode.

This project investigates whether **few-shot behavior can be distilled into model parameters** to:

* reduce early representation drift
* accelerate fine-tuning
* stabilize optimization.

---

## ğŸ¯ Core Hypothesis

Few-shot context performs an implicit low-rank weight update through attention.

If we supervise a zero-shot model using internal representations of a few-shot run:

* the model can learn to produce similar behavior **without context**.

---

## ğŸ§© Key Idea

### Teacher (Few-shot model)

```
Input = [Few-shot context + query]
Output = layer-wise attention activations
```

### Student (Zero-shot model)

```
Input = [query only]
Goal = match teacher internal representations
```

Training objective:

```
L = L_task + Î» * Î£ || h_l(student) âˆ’ h_l(teacher) ||Â²
```

Optional stronger formulation:

```
Î”h = h_fewshot âˆ’ h_zeroshot
```

Train student to predict adaptation signal Î”h.

---

# ğŸ§  Models

### **Qwen3-8B-Instruct**
### **Qwen3-4B-Instruct**
### **Qwen3-2B-Instruct**


# ğŸ§© Dataset Selection

The goal is NOT pure accuracy benchmarking.

We need datasets where:

```
few-shot performance >> zero-shot performance
```

so that adaptation signals are strong.

---

## â­ Primary Dataset â€” GSM8K

Math reasoning dataset chosen because:

* strong few-shot improvements
* structured reasoning signals
* attention layers carry meaningful computation
* widely accepted in ICL research.

This will be the main experimental environment.

---

## â­ Secondary Dataset â€” MMLU (subset)

Recommended subsets:

* logical reasoning
* abstract algebra
* professional law

Purpose:

* test generalization of learned adaptation.

---

## â­ Additional Dataset â€” BIG-Bench Hard (BBH)

Recommended tasks:

* causal reasoning
* logical deduction

Used for robustness validation.

---

## ğŸš€ Optional Extension

Multilingual reasoning tasks (future work):

* distilling few-shot adaptation across languages.

---

# ğŸ“‚ Project Structure

```
project/
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml
â”‚   â”œâ”€â”€ distill_layerwise.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ student.py
â”‚   â”‚   â”œâ”€â”€ teacher_wrapper.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_baseline.py
â”‚   â”‚   â”œâ”€â”€ train_layerwise_distill.py
â”‚   â”‚
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”œâ”€â”€ layer_matching.py
â”‚   â”‚
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ activation_capture.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ baseline_ft/
â”‚   â”œâ”€â”€ fewshot_teacher/
â”‚   â”œâ”€â”€ layerwise_distill/
â”‚
â””â”€â”€ README.md
```

---

# ğŸ§ª Experimental Plan

---

## Experiment 1 â€” Baseline Comparison

Compare:

| Method             | Description             |
| ------------------ | ----------------------- |
| Few-shot inference | Teacher baseline        |
| Zero-shot FT       | Standard fine-tuning    |
| Proposed           | Layer-wise distillation |

Key metric:

```
Iteration where FT loss < few-shot loss
```

---

## Experiment 2 â€” Layer Localization

Supervise:

* early layers
* middle layers
* late layers
* all layers

Goal:

> Identify where few-shot adaptation occurs.

---

## Experiment 3 â€” Attention vs FFN Matching

Variants:

* attention output matching
* FFN output matching
* full block matching

---

## Experiment 4 â€” Representation Drift Analysis

Track:

```
cosine(pretrained, current representations)
```

Questions:

* Does distillation reduce drift?
* Which layers drift most?

---

# ğŸ“Š Evaluation Metrics

### Optimization

* loss crossover iteration
* training stability

### Representation

* cosine similarity
* optional CKA / SVCCA

### Task

* task accuracy
* few-shot gap reduction

---

# âš™ï¸ Recommended Training Setup

Hardware:

```
4Ã— A100 GPUs
```

Suggested settings:

* LR warmup
* gradient clipping
* layer-wise LR decay (optional)

---

# ğŸ”¬ Data Formatting

Teacher input:

```
[Example 1 Q+A]
[Example 2 Q+A]
[Example 3 Q+A]
Target Question
```

Student input:

```
Target Question
```

Teacher/Student output (final loss calculated only on this):
```
Target Answer
```

---

# ğŸ§ª Vision

Transform:

```
Few-shot runtime adaptation
        â†“
Layer-wise context distillation
        â†“
Improved zero-shot model
```

Goal:

Reduce dependence on prompts while preserving adaptation capability.

---

# ğŸ¤ Contributors

* Amal Joe (IIT Bombay)